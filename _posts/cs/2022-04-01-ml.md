---
layout: post
title: 머신러닝 기초
author: An Hyungjun
categories: [AI]
tags: [ai, summary]
---

머신러닝 기초를 정리한다.

# 기초 용어

## 머신러닝과 데이터 사이언스
- 머신러닝: 데이터의 패턴을 찾아내는 과정
- 데이터 사이언스: 찾아낸 패턴에 의미를 부여하는 것

## 기계학습의 종류
- 지도학습: 정답 데이터가 있음
	- 회귀분석: 결과값을 예측하는 선을 찾는 것
	- 분류: 어떤 클래스에 속할 것인지를 판단하는 것
- 비지도학습: 정답 데이터가 없음
	- 클러스터링
- 강화학습: 에이전트가 보상을 최대화하는 (연속된)선택을 하도록 하는 방법

# 기초 선형 대수

## Scalar vs. Vector
- Scalar: 크기만 존재하는 양
- Vector: 크기와 방향이 함께 존재하는 양

## 벡터 공간 / 내적
- 놈(Norm): 벡터의 길이(원점으로부터의 거리), 제곱 합의 루트
- 내적(Euclidean inner product / Dot product): 각 원소 곱의 합

## 행렬
- 전치행렬: 행과 열을 뒤집음

# numpy
Python의 과학 컴퓨팅용 라이브러리

```python
import numpy an np

A = np.array([[11, 12],
			  [13, 14]])

B = np.array([[21, 22],
			  [23, 24]])

print(A)
```

## 원소 단위 계산
```python 
print(A * 2)
print(A ** 2)
print(2 ** A)
print(A * A)

print(A == B)
print(A > B)

np.logical_or(A1, A2)
np.logical_and(A1, A2)
```

## 행렬 단위 계산
```python
np.dot(A, B)

np.transpose(A)

np.linalg.inv(A)
```

## 스칼라화
```
np.sum(A)
A.sum(A)

A.min(A)
A.max(A)

A.argmin(A)
A.argmax(A)

np.all(A)
np.any(A)

np.linalg.norm(A)

np.mean(A)
np.median(A)
np.var(A)
np.std(A)
```

# 회귀분석
## 선형 회귀분석
- 데이터를 잘 설명하는 1차식 찾기
- 차이(Loss function)가 최소가 되게하는 계수 찾기
- Loss funtion의 최소점 찾기

## Loss function의 종류
- MSE(Mean Squared Error): 평균 제곱 오차, 차의 제곱의 합

## 다중선형 회귀분석
- 데이터가 2차원 이상

## 다항 회귀분석
- 데이터가 2차식 이상

# 베이지안

## 빈도주의 vs. 베이지안
- 빈도주의: 결과를 예측
- 베이지안: 확신 또는 믿음으로 해석

## 베이즈 법칙
- 잘 모르는 확률을 잘 아는 확률들로 구하기
```
P(A|X) = (P(X|A)P(A)) / P(X)
```

## Naive Bayes
- 어느 집단에 있을 확률이 더 높은가?
```
P(A|X), P(B|X)

P(X|A)P(A), P(X|B)P(B)

// P(A), P(B)는 사전확률
// P(X|A), P(X,B)를 우도(데이터를 얼마나 잘 설명하는지의 척도)
```

